\section{Künstliche Neuronale Netze}
\label{KNN}
\begin{document}
Die \textit{künstlichen neuronale Netze} oder auch \textit{künstlichen neuronale Netzwerke} (KNN) sind ein Verfahren im Bereich des Deep Learnings, womit bereits beachtliche Erfolge erzielt werden konnten. Aus enormen Datenmengen extrahieren die Netze \glqq Regelmäßigkeiten, Muster oder Modelle\grqq{} \cite[S. 40]{13}. 
Sie generieren also \glqq Wissen aus Erfahrung\grqq{} \cite[S. 40]{13} und \glqq der Computer [ist so in der Lage,] eigene Schlussfolgerungen aus diesem Wissen [zu] ziehen\grqq{} \cite[S. 33]{12}.
Hierdurch entdecken die \textit{KNN} nicht nur schneller Problemlösungen, sondern auch solche, die für den Menschen nicht zu erkennen sind. \cite[vgl.][S. 40]{13}

Ein \textit{künstliches neuronales Netz} setzt sich aus verschiedenen Schichten (engl. \textit{layers}) zusammen:

\begin{itemize}
	\item Zu Beginn des Netzes findet sich die \textit{Eingabeschicht} (engl. input layer). Diese repräsentiert die Merkmale, auch Feature genannt, des zu verarbeitenden Inputs als numerischen Wert. Es wird auch von einem $n$-dimensionalen Input- oder auch Merkmalsvektor $X$ mit den Werten $x\textsubscript{n}$ gesprochen [vlg.][S.176]\cite{14}. Im Falle eines Bildes beispielsweise entspricht ein Pixel einem Merkmal.
	\item Darauf folgen die \textit{verdeckten Schichten} (engl. hidden layer). Sie dienen der Weiterverarbeitung der Daten.
	\item den Abschluss bildet die \textit{Ausgabeschicht} (engl. output layer). Sie stellt die Zielwerte der Aufgabe dar. \\
	\cite[vgl. ][S.72]{12}
\end{itemize}
Die Schichten wiederum bestehen aus künstlichen Neuronen, die mittels einer gewichteten Verbindung verknüpft sind. Gewichtet bedeutet, dass auf ein Verbindung selbst einen Wert besitzt. Dieser Wert regelt dann \glqq den Anteil des Eingangswertes auf die Eingangssumme.\grqq{} \cite[S. 28]{13}. Die Gewichte können als Vektor $w \in \mathbb{R}^m$ abgebildet werden. Somit kann ein Input der nachfolgenden Schichten mittels Vektormultiplikation ermittelt werden. Die gesamten Gewichte eines Netzes werden als Modellparameter $\Theta$ ausgedrückt.

Nicht nur in einem Neuron, auch im gesamten Netz finden einige Vorgänge statt, die dazu beitragen, dass ein KNN so mächtig ist. Diese sollen nachfolgend näher gebracht werden.


\subsection{Das künstliche Neuron} \label{Neuron}
Wie bereits erwähnt, befinden sich künstliche Neuronen in den einzelnen Schichten eines \textit{künstlichen neuronalen Netzes}. Deren Anzahl variiert hierbei von Schicht zu Schicht, jedoch ist jedes Neuron einer Schicht mit allen Neuronen der folgenden Schicht gewichtet verbunden. Dabei ist der Input eines Neurons die Summe der gewichteten Outputs der vorgelagerten Neuronen \cite[vlg.][]{17}. Die Summe wird anschließend auf eine sogenannte \textit{Aktivierungsfunktion} (siehe Kapitel \ref{{Aktivierungsfunktion}) angewendet.
Damit stellt ein Neuron eine kleine \textit{Berechnungseinheit} dar \cite[vgl.][]{13}, die durch folgende Formel definiert ist:

\begin{equation}
y = f(\sum_{i=0}^{n} w\textsubscript{i}*x\textsubscript{i})
\end{equation} 

welche in Abbildung \ref{fig:Perzeptron} veranschaulicht dargestellt werden soll. x\textsubscript{n} beschreibt dabei einen Input, w\textsubscript{n} die Gewichtung und o\textsubscript{1} einen Output. 

\begin{figure}[H]
	\centering
	\includegraphics[width=0.3\textwidth]{img/Perzeptron.png}
	\caption{Perzeptron, eigene Darstellung}
	\label{fig:Perzeptron}
\end{figure}

Eine Ausnahme hiervon bildet die Eingabeschicht, denn wie bereits erwähnt, handelt es sich bei diesen Neuronen um den Merkmalsvektor, bei welchem ein Neuron einem Merkmal entspricht \cite[vgl.][S. 177]{14}. 
Die Eingabewerte [$x\textsubscript{n}$] bilden also unverändert die erste Schicht \cite[vlg.][S. 178]{14}. Nachfolgende Graphik zeigt ein extrem vereinfachtes KNN, anhand dessen die in den einzelnen Neuronen stattfindenden Vorgänge visualisiert:
\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\textwidth]{img/NN-Berechnung.png}
	\caption{Ein einfaches KNN, eigene Darstellung}
	\label{fig:NN-Berechnung}
\end{figure}
Abbildung \ref{fig:NN-Berechnung} zeigt, dass die Neuronen der Eingabeschicht lediglich einen Inputvektor der Größe $\mathbb{R}^3$ darstellen. Die mathematische Manipulation der Daten beginnt in der folgenden Schicht, der ersten verdeckten, und endet mit der Ausgabeschicht. In dieser befinden sich zwei Neuronen. Das bedeutet, dass \glqq aus einer dreidimensionalen Eingabe zwei Größen [prädiziert]\grqq{} \cite[vlg.][S. 174]{14} wurden. Der Wert der Prädiktion soll in kommendem Kapitel näher diskutiert werden.

\subsection{Aktivierungsfunktion} \label{Aktivierungsfunktion}
Ein Neuron leitet ein Signal nur dann weiter, wenn dieses einen gewissen Schwellenwert übersteigt. Dafür bildet die Summe des Eingabevektors mit den je zugehörigen Gewichtungen den Parameter der \textit{Aktivierungsfunktion}. Zusätzlich normalisiert diese die Werte, die durch das Netz fließen. So wird verhindert, dass ein drastisch unterschiedlicher Wertebereich entsteht (vlg. ID 18) 
Wird der Schwellenwert übertroffen, wird das Neuron aktiviert und das Ergebnis bildet den zu übertragenden Ausgang. Je nach Funktion können unterschiedliche Resultate erwartet werden. Die Bedeutendsten im Bereich der \textit{künstlichen neuronalen Netze} sind nachstehend mit zugehörigem Wertebereich aufgelistet.  
\cite[vgl.][S. 70]{12} \cite[vgl.][S. 35]{13} 


\begin{table}[h]
	\begin{tabular}[h]{p{1cm}|p{4cm}|p{3.9cm}|p{5.1cm}} 
		
		Index & Bezeichnung & Formel & Wertebereich der Ausgabe\\
		\hline
		\rule{0pt}{2em}
		1 & Sigmoid & ${f(x)= \frac {1}{1+e\textsuperscript{-x}}}$ & 0 bis 1\\
		\rule{0pt}{2em}
		2 & Rectified Linear Unit (ReLU) & ${f(x) = max(0,x)}$ & 
		$f(x)= \begin{cases}
		0, \mbox{ für } x < 0 \\ x, \mbox{ für } x \ge 0 
		\end{cases}$ \\
		\rule{0pt}{2em}
		3 & LeakyReLU & ${f(x) = max(\alpha x,x)}$, mit $\alpha$ = 0.01 &
		$f(x)= \begin{cases}
		0.01x, \mbox{ für } x < 0 \\ x, \mbox{ für } x \ge 0 
		\end{cases}$ \\
		\rule{0pt}{2em}
		4 & tanh & ${f(x)= \frac {e\textsuperscript{x} - e \textsuperscript{-x}} {e\textsuperscript{x} + e\textsuperscript{-x}}}$ & -1 bis 1 \\
		\rule{0pt}{2em}
		5 & Softmax & $f(x)= \frac {e\textsuperscript{xi}} {\sum_{j=1}^{N} e\textsuperscript{xj}} $ mit $ i = 1,...,N $  & 0 bis 1
	\end{tabular}
	\caption{Aktivierungsfunktion mit zugehöriger Formel und Wertebereich \\ Daten in Anlehnung an (vlg ID 18)
	\label{tab:Aktivierungsfunktion}
\end{table}


Die Auswahl der passenden Funktion sollte nicht nur von ihrem Wertebereich abhängen. Zu aktuellem Stand der Forschung ist nicht immer gegeben, dass eine spezielle Methode eine hinreichende Lösung für gleiche Probleme ist. Daher ist es ratsam, ein \textit{künstliches neuronales Netz} mit unterschiedlichen Methoden zu testen, trainieren und anschließend zu evaluieren.
\\
Daneben sollte die Wahl einer Funktion der Ausgabeschicht genauer betrachtet werden. Hier ist ein essenzieller Punkt, der zur Entscheidung beiträgt, das Klassifikationsproblem. Die Neuronen bilden hier die Zielwerte der Aufgabe ab. In den meisten Netzen finden sich die \textit{Sigmoidfunktion}, die \textit{tanh-Funktion} oder die \textit{Softmaxfunktion} in dem Output-Layer.




%\begin{itemize}
%	\item Zu \textbf{1}: Sie liefert für jeden Parameter einen Wert im Bereich zwischen 0 bis 1. Der Vorteil liegt nicht nur an der Einfachheit der Funktion, sondern auch an ihrer Ableitung, was in einer Zeitersparnis bei der Erstellung eines Modells resultiert. Doch die Ableitung bringt den Nachteil einer kurzen Reichweite mit sich, was bedeutet, dass hier Informationen verloren gehen können. Insbesondere in immer tiefer werdenden Netzen zeichnet sich hier ein immer stärkerer Informationsverlust aus. Es wird hier auch von einer Sättigungsgefahr gesprochen. \cite[vgl.][S. 212]{13} \cite[vgl.][]{18}
%	\item Zu \textbf{2}: Diese Methode liefert für Werte die unter Null liegen die Ausgabe 0. Alle anderen Werte werden unverändert weitergegeben.
%	\item \textbf{3}
%	\item \textbf{4}
%\end{itemize}
%(also Nettoeingabe, Aktivierungsfunktion, Normalisierung der letzten Schicht, z.B. Softmax, Output)


\subsection{Lernprozess}
Die Gewichtung bzw. Modellparameter werden durch den Lernprozess schrittweise angepasst. 

Wie in Abschnitt \ref{KNN} schon erwähnt, handelt es sich bei der Backpropagation um eine Lernregel im Bereich der KNN.

\subsubsection{Fehlerfunktion} 
\label{Fehlerfunktion}
Dabei muss zuerst ein Fehler mit Hilfe einer sogenannten Fehlerfunktion (engl. loss-function, cost-function oder objective function) ermittelt werden. Dieser Fehler wird anschließend durch das Netz geführt - der Gradientenabstieg. Das globale Minimum des Gradienten entspricht dabei dem Minimum des Fehlers.\\
\cite{10}
\subsubsection{Gradientenabstieg}
\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{img/Graph-Gewicht.png}
	\caption{Gradientenabstieg, eigene Darstellung}
	\label{fig:NN}
\end{figure}


\subsubsection{Herausforderungen des Lernens} 
\begin{itemize}
	\item \textbf{Globales Minimum}
	\item \textbf{Overfitting}
	\item \textbf{Vanishing Gradients}
	\item \textbf{Wahl der Hyperparameter}
	\glqq beruhe der Entwurf neuronaler Netze auf einer großen Willkür: Bei der Entscheidung, wie viele Lagen mit wie vielen Neuronen genutzt werden sollten, beruhe vieles auf Bauchgefühl oder auf Ausprobieren.\grqq{} \cite[S.44]{3}
	
\end{itemize}


\textbf{Optimizer}

\begin{itemize}
	\item \textbf{Momentum Optimierer} 
	\item \textbf{AdaGrad} 
	\item \textbf{RMSprop}  
	\item \textbf{Adam}  
\end{itemize}

\subsection{Arten von Künstlichen Neuronalen Netzen}
\subsubsection{Multilayer Perzeptron}
\glqq Die einfachste Version eines künstlichen neuronalen Netzes bildet das \textit{Multilayer Perceptron}\grqq{} \cite[S. 72]{12}, kurz MLP. 
Diese Art von Netz (hat denselben Aufbau wie Abbildung \ref{NN}) 
\glqq In der einfachsten Variante besteht [ein] Perzeptron aus einem einzigen Neuron mit einem Ausgang und mehreren Eingängen.\grqq{} \cite{15}

In den Schichten selbst befinden sich künstliche Neuronen, wobei die Anzahl dieser von Schicht zu Schicht variiert.
Ein Neuron bildet zusammen mit einem Output $o$ und $n \in N $ gewichteten Inputs ein Perzeptron. \cite{15} \cite[vgl.][ S. 81]{13}
Nachfolgend wird schematisch dargestellt, wie ein Perzeptron aufgebaut ist.


\textit{Multilayer Perzeptron} (MLP) schematisch dargestellt:
\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{img/NN.png}
	\caption{Struktur eines künstlichen neuronalen Netzes, eigene Darstellung}
	\label{fig:Netz}
\end{figure}


\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\textwidth]{img/NN-gelernt.png}
	\caption{Verbindungen eines trainierten Neuronalen Netzes bei einem bestimmten Input, eigene Darstellung}
	\label{fig:NN-gelernt}
\end{figure}




\subsubsection{Convolutional Neural Network}
\glqq CNN gehören zur Klasse der tiefen neuronalen Netze oder englisch Deep Neural Nets bzw. Deep-Learning-Verfahren\grqq{}
\cite{13}

\glqq Die Neuronen sind schichtweise angeordnet, wobei jede nachfolgende Schicht nur auf einen lokalen Bereich der Vorgängerschicht reagiert\grqq 


\end{document}